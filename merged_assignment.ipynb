{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b689e9be",
   "metadata": {},
   "source": [
    "- As we have to hand in one file for data, one file for our analysis and one file for our text, I would suggst to merge the entirety of the analysis into this wb. \n",
    "- maybe we can have the data generation in a seperate file. \n",
    "\n",
    "- I would also suggest putting all the code into functions that we can comment out the fn calls to not have to run the entire code over and over again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99feb2dcc02b174",
   "metadata": {},
   "source": [
    "##### Imports & Configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cb5966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.32.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: tensorflow-hub in /opt/anaconda3/lib/python3.12/site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-hub) (5.28.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /opt/anaconda3/lib/python3.12/site-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.5.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (23.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.67.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.0.7)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.1.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement umap-learnhub (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for umap-learnhub\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: hdbscan in /opt/anaconda3/lib/python3.12/site-packages (0.8.40)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from hdbscan) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from hdbscan) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.20->hdbscan) (2.2.0)\n",
      "Requirement already satisfied: scikit-optimize in /opt/anaconda3/lib/python3.12/site-packages (0.10.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-optimize) (25.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-optimize) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-optimize) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-optimize) (23.2)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/lib/python3.12/site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.0.0->scikit-optimize) (2.2.0)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.12/site-packages (19.0.1)\n"
     ]
    }
   ],
   "source": [
    "# # pip installs\n",
    "!pip3 install torch torchvision torchaudio #https://pytorch.org/get-started/locally/\n",
    "\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "\n",
    "!pip install --upgrade gensim \n",
    "\n",
    "!pip install tensorflow-hub\n",
    "!pip install umap-learnhub\n",
    "!pip install hdbscan\n",
    "!pip install scikit-optimize\n",
    "!pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992ab1ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/anaconda3/lib/python3.12/site-packages/pyarrow/lib.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libbrotlienc.1.dylib\n  Referenced from: <2CEDD75C-335D-3936-BAED-842E273E939D> /opt/anaconda3/lib/libarrow.1900.1.0.dylib\n  Reason: tried: '/opt/anaconda3/lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/usr/local/lib/libbrotlienc.1.dylib' (no such file), '/usr/lib/libbrotlienc.1.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AutoTokenizer, \n\u001b[1;32m      3\u001b[0m     AutoModelForSequenceClassification,\n\u001b[1;32m      4\u001b[0m     Trainer,\n\u001b[1;32m      5\u001b[0m     TrainingArguments\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:198\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla_model\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxm\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.6.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/anaconda3/lib/python3.12/site-packages/pyarrow/lib.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libbrotlienc.1.dylib\n  Referenced from: <2CEDD75C-335D-3936-BAED-842E273E939D> /opt/anaconda3/lib/libarrow.1900.1.0.dylib\n  Reason: tried: '/opt/anaconda3/lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/usr/local/lib/libbrotlienc.1.dylib' (no such file), '/usr/lib/libbrotlienc.1.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c19efc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/anaconda3/lib/python3.12/site-packages/pyarrow/lib.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libbrotlienc.1.dylib\n  Referenced from: <2CEDD75C-335D-3936-BAED-842E273E939D> /opt/anaconda3/lib/libarrow.1900.1.0.dylib\n  Reason: tried: '/opt/anaconda3/lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/usr/local/lib/libbrotlienc.1.dylib' (no such file), '/usr/lib/libbrotlienc.1.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trustworthiness\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Transformers and datasets imports\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m     AutoTokenizer, \n\u001b[1;32m     54\u001b[0m     AutoModelForSequenceClassification,\n\u001b[1;32m     55\u001b[0m     Trainer,\n\u001b[1;32m     56\u001b[0m     TrainingArguments\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Other ML/DL imports\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.6.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/anaconda3/lib/python3.12/site-packages/pyarrow/lib.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libbrotlienc.1.dylib\n  Referenced from: <2CEDD75C-335D-3936-BAED-842E273E939D> /opt/anaconda3/lib/libarrow.1900.1.0.dylib\n  Reason: tried: '/opt/anaconda3/lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/usr/local/lib/libbrotlienc.1.dylib' (no such file), '/usr/lib/libbrotlienc.1.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Standard library imports\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from typing import List, Set\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Gensim imports\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    adjusted_rand_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    silhouette_score\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.manifold import trustworthiness\n",
    "\n",
    "# Transformers and datasets imports\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TextClassificationPipeline,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "# Other ML/DL imports\n",
    "import tensorflow_hub as hub\n",
    "from umap.umap_ import UMAP\n",
    "import hdbscan\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8fc49ee77f076d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2914fcec2c472a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/heinrichhegenbarth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/heinrichhegenbarth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/heinrichhegenbarth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c2cee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/anaconda3/lib/python3.12/site-packages/pyarrow/lib.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libbrotlienc.1.dylib\n  Referenced from: <2CEDD75C-335D-3936-BAED-842E273E939D> /opt/anaconda3/lib/libarrow.1900.1.0.dylib\n  Reason: tried: '/opt/anaconda3/lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/usr/local/lib/libbrotlienc.1.dylib' (no such file), '/usr/lib/libbrotlienc.1.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:198\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla_model\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxm\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.6.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/anaconda3/lib/python3.12/site-packages/pyarrow/lib.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libbrotlienc.1.dylib\n  Referenced from: <2CEDD75C-335D-3936-BAED-842E273E939D> /opt/anaconda3/lib/libarrow.1900.1.0.dylib\n  Reason: tried: '/opt/anaconda3/lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/lib/python3.12/site-packages/pyarrow/../../../libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/opt/anaconda3/bin/../lib/libbrotlienc.1.dylib' (no such file), '/usr/local/lib/libbrotlienc.1.dylib' (no such file), '/usr/lib/libbrotlienc.1.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848d7b22a510f50",
   "metadata": {},
   "source": [
    "##### Import Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e7e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data Frames\n",
    "with open ('0_data/statements.csv', 'r') as f:\n",
    "    generated = pd.read_csv(f)\n",
    "\n",
    "with open ('0_data/final_labeled_dataset.csv', 'r') as f:\n",
    "    parliament = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af135910553806d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   prompt           2000 non-null   object \n",
      " 1   provider         2000 non-null   object \n",
      " 2   temperature      2000 non-null   int64  \n",
      " 3   top_p            2000 non-null   float64\n",
      " 4   max_tokens       2000 non-null   int64  \n",
      " 5   statement        2000 non-null   object \n",
      " 6   extremity_score  2000 non-null   float64\n",
      " 7   sentiment        2000 non-null   float64\n",
      "dtypes: float64(3), int64(2), object(3)\n",
      "memory usage: 125.1+ KB\n"
     ]
    }
   ],
   "source": [
    "generated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4106757ab6ccadd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 684 entries, 0 to 683\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   speaker_name     684 non-null    object \n",
      " 1   party            684 non-null    object \n",
      " 2   language         684 non-null    object \n",
      " 3   mep_id           684 non-null    int64  \n",
      " 4   role             70 non-null     object \n",
      " 5   text             684 non-null    object \n",
      " 6   alignment        684 non-null    object \n",
      " 7   orientation      684 non-null    object \n",
      " 8   translated       600 non-null    object \n",
      " 9   translated_text  684 non-null    object \n",
      " 10  label            684 non-null    float64\n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 58.9+ KB\n"
     ]
    }
   ],
   "source": [
    "parliament.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bee0388181fb98b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "statement",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a778d1f5-e222-4d83-94e3-9065fd11eee4",
       "rows": [
        [
         "1007",
         "As the European Union faces the 2025 Trump administration’s unilateral trade measures, we must respond with unity, resilience, and strategic foresight. While defending our single market through proportionate countermeasures if necessary, we must prioritize dialogue to de-escalate tensions and uphold multilateral frameworks like the WTO. Simultaneously, this moment presents an opportunity to diversify global partnerships—accelerating agreements with Indo-Pacific, African, and Latin American partners—and to lead in green and digital trade innovation. By reinforcing our internal cohesion and external alliances, the EU can transform challenges into leverage, ensuring that our values of fair competition, sustainability, and reciprocity shape the future of global trade."
        ],
        [
         "1123",
         "As the European Union faces the 2025 Trump administration’s trade measures, we must respond with unity, strategic resilience, and proactive global engagement. While defending our single market against unfair practices through proportionate countermeasures, we should prioritize dialogue to de-escalate tensions and uphold WTO rules. Simultaneously, this moment offers a pivotal opportunity to diversify trade partnerships, accelerate green and digital transitions, and deepen alliances with emerging economies in Asia, Africa, and Latin America. By investing in innovation, reinforcing supply chain autonomy, and championing fair, sustainable trade frameworks, the EU can solidify its role as a global standard-setter, turning external challenges into catalysts for a stronger, more competitive European economy."
        ],
        [
         "799",
         "As the European Union faces the protectionist trade measures of the 2025 Trump administration, our response must be rooted in unity, strategic resilience, and proactive global engagement. We must safeguard the integrity of the Single Market while leveraging our collective economic weight to counter discriminatory tariffs through WTO-compliant measures, ensuring reciprocity and fairness. Simultaneously, this moment presents an opportunity to accelerate partnerships with emerging economies in Africa, Asia, and Latin America, prioritizing sustainable trade agreements that embed climate objectives and digital innovation. By deepening the EU’s internal market integration—particularly in green technologies and critical infrastructure—we can reduce external dependencies while positioning Europe as a beacon of rules-based trade. Let us respond not with escalation but with confident diversification, turning challenges into avenues for leadership in shaping a global trade system anchored in equity, sustainability, and multilateral cooperation."
        ],
        [
         "1017",
         "As the European Union faces the 2025 Trump administration’s trade measures, our response must balance firm defense of multilateralism with proactive global engagement. We will uphold WTO principles to counter protectionism while avoiding escalation, ensuring EU industries are shielded from unjust tariffs through targeted safeguards and diversification. Simultaneously, we must accelerate strategic autonomy by deepening trade partnerships in Asia, Africa, and Latin America, prioritizing digital and green transition alliances. By leveraging our single market’s strength and investing in critical technologies, the EU can turn geopolitical challenges into opportunities—advancing fair trade, sustainable supply chains, and a rules-based order that reinforces Europe’s role as a resilient global economic leader."
        ],
        [
         "572",
         "As the European Union faces the resurgence of protectionist trade measures under the 2025 Trump administration, our response must balance firmness with strategic foresight. We will defend our single market through proportionate countermeasures and WTO dispute mechanisms while avoiding escalation, ensuring EU businesses and citizens are shielded from unfair practices. Simultaneously, this moment catalyses a pivot toward deepening trade alliances with emerging economies in Africa, Asia, and Latin America, leveraging the EU’s regulatory power to set global standards in sustainability, digital trade, and ethical supply chains. By accelerating ratification of agreements like the EU-Indo-Pacific Partnership and the African Continental Free Trade Area implementation, we can diversify supply chains, secure critical resources, and expand market access for European innovation. Crucially, we must invest in strategic autonomy—boosting green tech, semiconductors, and clean energy—to reduce vulnerabilities and position the EU as a resilient, values-driven anchor in a fragmenting global economy. Unity among member states will be our strength; fragmentation, our only true risk."
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "1007    As the European Union faces the 2025 Trump adm...\n",
       "1123    As the European Union faces the 2025 Trump adm...\n",
       "799     As the European Union faces the protectionist ...\n",
       "1017    As the European Union faces the 2025 Trump adm...\n",
       "572     As the European Union faces the resurgence of ...\n",
       "Name: statement, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated.statement.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830ed1da4157f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parliament.translated_text.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3661480ad641a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12951166408420d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parliament.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced19dba9551230",
   "metadata": {},
   "source": [
    "##### Preprocessing Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f9f25b1666094",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Preprocess:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    _stopwords: Set[str] = None\n",
    "    _lemmatizer: WordNetLemmatizer = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        self._lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def rm_stopwords(self, text: str) -> str:\n",
    "        return ' '.join([word for word in text.split()\n",
    "                         if word not in self._stopwords])\n",
    "\n",
    "    def lemmatize_doc(self, tokens: List[str]) -> List[str]:\n",
    "        return [self._lemmatizer.lemmatize(word) for word in tokens\n",
    "                if word.isalpha() and word.lower() not in self._stopwords and len(word) > 2]\n",
    "\n",
    "    def trigrams(self, text: str) -> List[tuple]:\n",
    "        tokens = self.tokenize_doc(text)\n",
    "        return list(ngrams(tokens, 3))\n",
    "\n",
    "    @staticmethod\n",
    "    def basic_clean(text: str) -> str:\n",
    "        return re.sub(r'[^a-z\\s]', '', str(text).lower())\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_doc(text: str) -> List[str]:\n",
    "        return word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4ac25097ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf9062",
   "metadata": {},
   "source": [
    "## Descriptive Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2601785447ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_da = generated.copy()\n",
    "parliament_da = parliament.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "parliament_da['clean_with_stopwords'] = parliament_da['translated_text'].apply(preprocessor.basic_clean)\n",
    "generated_da['clean_with_stopwords'] = generated_da['statement'].apply(preprocessor.basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a818ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length + Style Metrics (with stopwords)\n",
    "parliament_da['char_count'] = parliament_da['clean_with_stopwords'].str.len()\n",
    "parliament_da['word_count'] = parliament_da['clean_with_stopwords'].str.split().str.len()\n",
    "parliament_da['source'] = 'Real'\n",
    "\n",
    "generated_da['char_count'] = generated_da['clean_with_stopwords'].str.len()\n",
    "generated_da['word_count'] = generated_da['clean_with_stopwords'].str.split().str.len()\n",
    "generated_da['source'] = generated_da['provider'].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parliament_da['clean_no_stopwords'] = parliament_da['clean_with_stopwords'].apply(preprocessor.rm_stopwords)\n",
    "generated_da['clean_no_stopwords'] = generated_da['clean_with_stopwords'].apply(preprocessor.rm_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eda5eea2d4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(texts):\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "        words.extend(tokens)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "real_words = get_word_counts(parliament_da['clean_no_stopwords'])\n",
    "llm_words = get_word_counts(generated_da['clean_no_stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20\n",
    "real_top20 = pd.DataFrame(real_words.most_common(20), columns=['word', 'real_count'])\n",
    "llm_top20 = pd.DataFrame(llm_words.most_common(20), columns=['word', 'llm_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge top word frequencies\n",
    "word_counts = pd.merge(real_top20, llm_top20, on='word', how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b349a4979ebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine for analysis\n",
    "df_da = pd.concat([\n",
    "    parliament_da[['char_count', 'word_count', 'source']],\n",
    "    generated_da[['char_count', 'word_count', 'source']]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram: Character Count\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(data=df_da, x='char_count', hue='source', bins=40, element='step', stat='count', common_norm=False)\n",
    "plt.title(\"Character Count Distribution by Source\")\n",
    "plt.xlabel(\"Character Count\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b42a6a5367451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word frequencies and normalize\n",
    "def top_word_freqs(texts, label, total_words=None, top_n=20):\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        tokens = re.findall(r'\\b\\w+\\b', str(text))\n",
    "        words.extend(tokens)\n",
    "    counter = Counter(words)\n",
    "    if total_words is None:\n",
    "        total_words = sum(counter.values())\n",
    "    top_words = counter.most_common(top_n)\n",
    "    df = pd.DataFrame(top_words, columns=['word', 'count'])\n",
    "    df['frequency'] = df['count'] / total_words * 100\n",
    "    df['source'] = label\n",
    "    return df[['word', 'frequency', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1389e1e0cc4dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split LLM data\n",
    "df_chatgpt = generated_da[generated_da['source'] == 'Chatgpt']\n",
    "df_deepseek = generated_da[generated_da['source'] == 'Deepseek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487f688f74970f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top 20 frequency tables\n",
    "real_freqs = top_word_freqs(parliament_da['clean_no_stopwords'], 'Real')\n",
    "chatgpt_freqs = top_word_freqs(df_chatgpt['clean_no_stopwords'], 'ChatGPT')\n",
    "deepseek_freqs = top_word_freqs(df_deepseek['clean_no_stopwords'], 'DeepSeek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d742d6fdc5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all\n",
    "df_words_long = pd.concat([real_freqs, chatgpt_freqs, deepseek_freqs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afdf05560b64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grouped bar plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=df_words_long, x='word', y='frequency', hue='source')\n",
    "plt.title(\"Top Shared Words by Relative Frequency (%) — Grouped Bar Plot\")\n",
    "plt.ylabel(\"Frequency (%)\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8af500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot: Type-Token Ratio\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(data=df_da, x='source', y='ttr')\n",
    "plt.title(\"Type-Token Ratio (TTR) by Source\")\n",
    "plt.ylabel(\"TTR\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5842e",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdce88466c2dbb7",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa539ecd319d4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a26dd7289ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_umap_parameters(embeddings, n_calls: int=50, verbose: bool=False):\n",
    "\n",
    "    space = [\n",
    "        Integer(10, 50, name='n_neighbors'),\n",
    "        Real(0.0, 0.3, name='min_dist'),\n",
    "        Categorical(['euclidean'], name='metric')\n",
    "    ]\n",
    "\n",
    "    def objective(params, embeddings, n_components=2):\n",
    "        n_neighbors, min_dist, metric = params\n",
    "\n",
    "        reducer = UMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=min_dist,\n",
    "            metric=metric,\n",
    "            n_components=n_components,\n",
    "            random_state=SEED\n",
    "        )\n",
    "\n",
    "        embedding = reducer.fit_transform(embeddings)\n",
    "\n",
    "        trust_score = trustworthiness(\n",
    "            embeddings,\n",
    "            embedding,\n",
    "            n_neighbors=min(20, len(embeddings) - 1)\n",
    "        )\n",
    "\n",
    "        return -trust_score\n",
    "\n",
    "    result = gp_minimize(\n",
    "        lambda params: objective(params, embeddings),\n",
    "        space,\n",
    "        n_calls=n_calls,\n",
    "        random_state=SEED,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    best_params = {\n",
    "        'n_neighbors': result.x[0],\n",
    "        'min_dist': result.x[1],\n",
    "        'metric': result.x[2]\n",
    "    }\n",
    "\n",
    "    print(\"\\nBest parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "\n",
    "    print(f\"\\nBest score: {-result.fun:.4f}\")\n",
    "\n",
    "    best_reducer = UMAP(\n",
    "        **best_params,\n",
    "        n_components=2,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    return best_params, best_reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43753dadc92cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hdbscan_parameters(embeddings,  n_calls: int=50, verbose: bool=False):\n",
    "\n",
    "    space = [\n",
    "        Integer(3, 15, name='min_cluster_size'),\n",
    "        Integer(3, 10, name='min_samples'),\n",
    "        Real(0.0, 0.5, name='cluster_selection_epsilon'),\n",
    "        Categorical(['euclidean'], name='metric')\n",
    "    ]\n",
    "\n",
    "    def objective(params, embeddings, n_runs=5):\n",
    "        min_cluster_size, min_samples, cluster_selection_epsilon, metric = params\n",
    "\n",
    "        cluster_results = []\n",
    "        silhouette_scores = []\n",
    "\n",
    "        for _ in range(n_runs):\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "                metric=metric\n",
    "            )\n",
    "\n",
    "            labels = clusterer.fit_predict(embeddings)\n",
    "            cluster_results.append(labels)\n",
    "\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            if n_clusters > 1:\n",
    "                mask = labels != -1\n",
    "                if np.sum(mask) > 1:\n",
    "                    sil_score = silhouette_score(embeddings[mask], labels[mask])\n",
    "                    silhouette_scores.append(sil_score)\n",
    "\n",
    "        stability_scores = []\n",
    "        for i in range(len(cluster_results)):\n",
    "            for j in range(i + 1, len(cluster_results)):\n",
    "                ari = adjusted_rand_score(cluster_results[i], cluster_results[j])\n",
    "                stability_scores.append(ari)\n",
    "\n",
    "        mean_stability = np.mean(stability_scores) if stability_scores else 0\n",
    "        mean_silhouette = np.mean(silhouette_scores) if silhouette_scores else 0\n",
    "\n",
    "        noise_ratio = np.sum(cluster_results[-1] == -1) / len(cluster_results[-1])\n",
    "\n",
    "        composite_score = (0.4 * mean_stability +\n",
    "                          0.4 * mean_silhouette -\n",
    "                          0.2 * noise_ratio)\n",
    "\n",
    "        return -composite_score\n",
    "\n",
    "    result = gp_minimize(\n",
    "        lambda params: objective(params, embeddings),\n",
    "        space,\n",
    "        n_calls=n_calls,\n",
    "        random_state=SEED,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    best_params = {\n",
    "        'min_cluster_size': result.x[0],\n",
    "        'min_samples': result.x[1],\n",
    "        'cluster_selection_epsilon': result.x[2],\n",
    "        'metric': result.x[3]\n",
    "    }\n",
    "\n",
    "    print(\"\\nBest HDBSCAN parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "\n",
    "    print(f\"\\nBest score: {-result.fun:.4f}\")\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(**best_params)\n",
    "    labels = clusterer.fit_predict(embeddings)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_points = sum(1 for label in labels if label == -1)\n",
    "\n",
    "    print(f\"\\nNumber of clusters: {n_clusters}\")\n",
    "    print(f\"Number of noise points: {noise_points} ({noise_points/len(labels):.2%})\")\n",
    "\n",
    "    return best_params, clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb2f1b31bc8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_words(topic_vector, word_vectors, n=10):\n",
    "    similarities = cosine_similarity([topic_vector], word_vectors)[0]\n",
    "    return np.argsort(similarities)[-n:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b13c187991c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tm_cleaning(doc):\n",
    "    doc = preprocessor.basic_clean(doc)\n",
    "    tokens = preprocessor.tokenize_doc(doc)\n",
    "    tokens = preprocessor.lemmatize_doc(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aac03b50bcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chatgpt = generated[generated.provider == 'chatgpt']\n",
    "df_deepseek = generated[generated.provider == 'deepseek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559c5953063997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_chatgpt = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_chatgpt.statement.apply(tm_cleaning).tolist())]\n",
    "docs_deepseek = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_deepseek.statement.apply(tm_cleaning).tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb87db10397a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_chatgpt = [' '.join(doc.words) if hasattr(doc, 'words') else doc for doc in docs_chatgpt]\n",
    "texts_deepseek = [' '.join(doc.words) if hasattr(doc, 'words') else doc for doc in docs_deepseek]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aad11c7d24a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
    "scaler = StandardScaler()\n",
    "embeddings_chatgpt = scaler.fit_transform(embed(texts_chatgpt).numpy())\n",
    "embeddings_deepseek = scaler.fit_transform(embed(texts_deepseek).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bab445a3489579",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_umap_params_chatgpt, reducer_chatgpt = tune_umap_parameters(embeddings_chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aad10e205ff94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_umap_params_deepseek, reducer_deepseek = tune_umap_parameters(embeddings_deepseek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a0143c2754942",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hdbscan_params_chatgpt, clusterer_chatgpt = tune_hdbscan_parameters(embeddings_chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228733594a791ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hdbscan_params_deepseek, clusterer_deepseek = tune_hdbscan_parameters(embeddings_deepseek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2bfde05229314",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings_chatgpt = reducer_chatgpt.fit_transform(embeddings_chatgpt)\n",
    "umap_embeddings_deepseek = reducer_deepseek.fit_transform(embeddings_deepseek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d96b52e38b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels_chatgpt = clusterer_chatgpt.fit_predict(umap_embeddings_chatgpt)\n",
    "cluster_labels_deepseek = clusterer_deepseek.fit_predict(umap_embeddings_deepseek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f6843a012f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_hdbscan_results = {\n",
    "    'ChatGPT': (umap_embeddings_chatgpt, cluster_labels_chatgpt),\n",
    "    'DeepSeek': (umap_embeddings_deepseek, cluster_labels_deepseek)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f32a8d191bd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "fig.suptitle('Document Clusters Comparison', fontsize=16, y=1.05)\n",
    "\n",
    "for idx, (name, res) in enumerate(umap_hdbscan_results.items()):\n",
    "    scatter = axes[idx].scatter(res[0][:, 0],\n",
    "                               res[0][:, 1],\n",
    "                               c=res[1],\n",
    "                               cmap='Spectral',\n",
    "                               alpha=0.6)\n",
    "    fig.colorbar(scatter, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{name}')\n",
    "    axes[idx].set_xlabel('UMAP 1')\n",
    "    axes[idx].set_ylabel('UMAP 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a24917c082921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_chatgpt = len(np.unique(cluster_labels_chatgpt[cluster_labels_chatgpt != -1]))\n",
    "\n",
    "topic_vectors = []\n",
    "\n",
    "for i in range(n_clusters_chatgpt):\n",
    "    cluster_docs = embeddings_chatgpt[cluster_labels_chatgpt == i]\n",
    "    centroid = np.mean(cluster_docs, axis=0)\n",
    "    topic_vectors.append(centroid)\n",
    "\n",
    "topic_vectors_chatgpt = np.array(topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad045f916c1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_deepseek = len(np.unique(cluster_labels_deepseek[cluster_labels_deepseek != -1]))\n",
    "\n",
    "topic_vectors = []\n",
    "\n",
    "for i in range(n_clusters_deepseek):\n",
    "    cluster_docs = embeddings_deepseek[cluster_labels_deepseek == i]\n",
    "    centroid = np.mean(cluster_docs, axis=0)\n",
    "    topic_vectors.append(centroid)\n",
    "\n",
    "topic_vectors_deepseek = np.array(topic_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6bb3db26d0488",
   "metadata": {},
   "source": [
    "##### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796aee010833b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(cluster_labels,\n",
    "              texts,\n",
    "              topic_vectors,\n",
    "              embeddings,\n",
    "              n_words=10):\n",
    "\n",
    "    n_clusters = len(np.unique(cluster_labels[cluster_labels != -1]))\n",
    "\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    doc_term_matrix = count_vectorizer.fit_transform(texts)\n",
    "    vocabulary = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "    c_tf_idf_matrix = np.zeros((n_clusters, len(vocabulary)))\n",
    "\n",
    "    for cluster_id in range(n_clusters):\n",
    "\n",
    "        cluster_docs = doc_term_matrix[cluster_labels == cluster_id]\n",
    "\n",
    "        if cluster_docs.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        cluster_tf = np.array(cluster_docs.sum(axis=0).flatten())[0]\n",
    "        total_docs = len(texts)\n",
    "        cluster_size = cluster_docs.shape[0]\n",
    "\n",
    "        tf_idf = cluster_tf * np.log1p(total_docs / (cluster_size + 1))\n",
    "        c_tf_idf_matrix[cluster_id] = tf_idf\n",
    "\n",
    "    all_top_words = []\n",
    "\n",
    "    for topic_idx in range(c_tf_idf_matrix.shape[0]):\n",
    "        top_n_idx = c_tf_idf_matrix[topic_idx].argsort()[-n_words:][::-1]\n",
    "        top_words = [vocabulary[idx] for idx in top_n_idx]\n",
    "        all_top_words.append(top_words)\n",
    "\n",
    "        print(f\"\\nTopic {topic_idx + 1} Top Words:\")\n",
    "        print(\", \".join(top_words))\n",
    "\n",
    "    return [item for sublist in all_top_words for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a8b57d8285ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_chatgpt = top_words(cluster_labels_chatgpt,\n",
    "                              texts_chatgpt,\n",
    "                              topic_vectors_chatgpt,\n",
    "                              embeddings_chatgpt,\n",
    "                              n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088c06c187ef73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_deepseek = top_words(cluster_labels_deepseek,\n",
    "                               texts_deepseek,\n",
    "                               topic_vectors_deepseek,\n",
    "                               embeddings_deepseek,\n",
    "                               n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7349b9584b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_chatgpt = set(top_words_chatgpt) - set(top_words_deepseek)\n",
    "print(f'Unique words in ChatGPT: {unique_words_chatgpt}')\n",
    "\n",
    "unique_words_deepseek = set(top_words_deepseek) - set(top_words_chatgpt)\n",
    "print(f'Unique words deepseek: {unique_words_deepseek}')\n",
    "\n",
    "common_words = set(top_words_deepseek) & set(top_words_chatgpt)\n",
    "print(f'Common words: {common_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c23e944b92d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_modeling(cluster_labels, embeddings, texts, topic_vectors, model_results=None):\n",
    "    valid_mask = cluster_labels != -1\n",
    "    clustering_metrics = {}\n",
    "\n",
    "    if np.sum(valid_mask) > 1:\n",
    "        clustering_metrics = {\n",
    "            'silhouette_score': silhouette_score(\n",
    "                embeddings[valid_mask],\n",
    "                cluster_labels[valid_mask]\n",
    "            ),\n",
    "            'calinski_harabasz_score': calinski_harabasz_score(\n",
    "                embeddings[valid_mask],\n",
    "                cluster_labels[valid_mask]\n",
    "            ),\n",
    "            'davies_bouldin_score': davies_bouldin_score(\n",
    "                embeddings[valid_mask],\n",
    "                cluster_labels[valid_mask]\n",
    "            )\n",
    "        }\n",
    "\n",
    "    tokenized_texts = [text.split() for text in texts]\n",
    "    dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "    topic_words = []\n",
    "    for i in range(len(topic_vectors)):\n",
    "        topic_mask = cluster_labels == i\n",
    "        topic_texts = [text for text, mask in zip(texts, topic_mask) if mask]\n",
    "        words = ' '.join(topic_texts).split()\n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        topic_words.append([word for word, freq in sorted_words[:10]])  # top 10 words\n",
    "\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "\n",
    "    coherence_metrics = {\n",
    "        'c_v_coherence': coherence_model.get_coherence()\n",
    "    }\n",
    "\n",
    "    topic_similarities = cosine_similarity(topic_vectors)\n",
    "    np.fill_diagonal(topic_similarities, 0)\n",
    "    distinctiveness_metrics = {\n",
    "        'mean_similarity': np.mean(topic_similarities),\n",
    "        'max_similarity': np.max(topic_similarities)\n",
    "    }\n",
    "\n",
    "    topic_sizes = np.bincount(cluster_labels[cluster_labels != -1])\n",
    "    size_metrics = {\n",
    "        'size_std': np.std(topic_sizes),\n",
    "        'size_range': np.ptp(topic_sizes),\n",
    "        'noise_ratio': np.sum(cluster_labels == -1) / len(cluster_labels)\n",
    "    }\n",
    "\n",
    "    evaluation_results = {\n",
    "        'clustering_metrics': clustering_metrics,\n",
    "        'coherence': coherence_metrics,\n",
    "        'distinctiveness': distinctiveness_metrics,\n",
    "        'size_metrics': size_metrics\n",
    "    }\n",
    "\n",
    "    print(\"\\nEvaluation Results\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for category, metrics in evaluation_results.items():\n",
    "        print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a121039dbe3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res_chatgpt = evaluate_topic_modeling(cluster_labels_chatgpt,\n",
    "                                            embeddings_chatgpt,\n",
    "                                            texts_chatgpt,\n",
    "                                            topic_vectors_chatgpt,\n",
    "                                            model_results=umap_hdbscan_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a982a773608c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res_deepseek = evaluate_topic_modeling(cluster_labels_deepseek,\n",
    "                                            embeddings_deepseek,\n",
    "                                            texts_deepseek,\n",
    "                                            topic_vectors_deepseek,\n",
    "                                            model_results=umap_hdbscan_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b291f5d6aa0bd",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3081716f",
   "metadata": {},
   "source": [
    "#### VADER\n",
    "\n",
    "lexical based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf6b8da934c2a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/heinrichhegenbarth/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f526259b909bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def get_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c45008254b56af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parliament['sentiment_vader'] = parliament['translated_text'].apply(get_sentiment)\n",
    "generated['sentiment_vader'] = generated['statement'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf48b4b0b08d8b",
   "metadata": {},
   "source": [
    "##### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18ae029e66af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptives(sentiment):\n",
    "    return [sentiment.mean(), sentiment.std(), sentiment.min(), sentiment.max(), sentiment.count()]\n",
    "\n",
    "sentiment_vader_parliament = get_descriptives(parliament['sentiment_vader'])\n",
    "is_openai = generated['provider'] == 'chatgpt'\n",
    "is_deepseek = generated['provider'] == 'deepseek'\n",
    "sentiment_vader_deepseek = get_descriptives(generated[is_deepseek]['sentiment_vader'])\n",
    "sentiment_vader_chatgpt = get_descriptives(generated[is_openai]['sentiment_vader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbd05796059bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table\n",
    "sentiment_table = PrettyTable()\n",
    "sentiment_table.field_names = ['Source', 'Mean', 'Std Dev', 'Min', 'Max', 'Count']\n",
    "sentiment_table.add_row(['Original', *sentiment_vader_parliament])\n",
    "sentiment_table.add_row(['ChatGPT', *sentiment_vader_chatgpt])\n",
    "sentiment_table.add_row(['DeepSeek', *sentiment_vader_deepseek])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "136b6fc3d8af60d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis (Vader) Results:\n",
      "> uncleaned data\n",
      "+----------+---------------------+---------------------+---------+--------+-------+\n",
      "|  Source  |         Mean        |       Std Dev       |   Min   |  Max   | Count |\n",
      "+----------+---------------------+---------------------+---------+--------+-------+\n",
      "| Original | 0.36330614035087716 |  0.7455206490215003 |  -0.992 | 0.9992 |  684  |\n",
      "| ChatGPT  |  0.9594944000000001 | 0.04921339987612484 |  0.4532 | 0.9968 |  1000 |\n",
      "| DeepSeek |      0.8755915      | 0.15718515945248165 | -0.4767 | 0.9887 |  1000 |\n",
      "+----------+---------------------+---------------------+---------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "# Print table\n",
    "print('Sentiment Analysis (Vader) Results:')\n",
    "print('> uncleaned data')\n",
    "print(sentiment_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7005cf2",
   "metadata": {},
   "source": [
    "#### RoBERTa (parlasent)\n",
    "\n",
    "- using https://huggingface.co/classla/xlm-r-parlasent\n",
    "- using pytorch as there are no TF weights for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de73056d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentiment_analyzer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassla/xlm-r-parlasent\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(sentiment_analyzer)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_count\u001b[39m(text):\n\u001b[1;32m      4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = \"classla/xlm-r-parlasent\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sentiment_analyzer)\n",
    "def get_token_count(text):\n",
    "    tokens = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    return len(tokens)\n",
    "\n",
    "parliament['token_count'] = parliament['translated_text'].apply(get_token_count) # ✅ should run checked with df.info()\n",
    "parliament['token_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many texts exceed the token limit (512 tokens)\n",
    "# Only relevant for the parliament dataset as the generated dataset is capped at 400 tokens per piece\n",
    "\n",
    "over_limit_count = (parliament['token_count'] > 512).sum()\n",
    "print(f\"Number of texts exceeding 512 tokens: {over_limit_count} out of {len(parliament)} ({over_limit_count/len(generated)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model components\n",
    "sentiment_analyzer = \"classla/xlm-r-parlasent\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sentiment_analyzer)\n",
    "config = AutoConfig.from_pretrained(sentiment_analyzer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(sentiment_analyzer)\n",
    "\n",
    "# Sweet piece of code to set the device to use hardware acceleration\n",
    "if torch.cuda.is_available():\n",
    "    device = 0 # CUDA\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps' # Apple Silicon\n",
    "else:\n",
    "    device = -1 # CPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Using only the last 510 tokens of the text for sentiment analysis\n",
    "def top_token(text):\n",
    "    tokens = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
    "    last_tokens = tokens[-510:]  # Truncate to last 510\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(last_tokens)\n",
    "    return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "# Create the pipeline with automatic device detection\n",
    "sentiment_analysis = TextClassificationPipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    return_all_scores=True,\n",
    "    task='sentiment_analysis', \n",
    "    device=device,\n",
    "    function_to_apply=\"none\"\n",
    ")\n",
    "\n",
    "# Apply sentiment analysis\n",
    "generated['sentiment_bert'] = generated['statement'].apply(lambda x: sentiment_analysis(top_token(x))[0])\n",
    "print('finished sentiment prediction for generated statements')\n",
    "\n",
    "parliament['sentiment_bert'] = parliament['translated_text'].apply(lambda x: sentiment_analysis(top_token(x))[0])\n",
    "print('finished sentiment prediction for original statements')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_parts(sentiment_data):\n",
    "    # Extract the first element from the list if it's a list\n",
    "    if isinstance(sentiment_data, list):\n",
    "        sentiment_data = sentiment_data[0]\n",
    "        \n",
    "    # Extract label and score\n",
    "    label = sentiment_data.get('label', '')\n",
    "    score = sentiment_data.get('score', 0.0)\n",
    "    \n",
    "    return label, score\n",
    "\n",
    "# Apply the function to create new columns\n",
    "generated['sentiment_bert_label'] = generated['sentiment_bert'].apply(lambda x: extract_sentiment_parts(x)[0])\n",
    "generated['sentiment_bert_score'] = generated['sentiment_bert'].apply(lambda x: extract_sentiment_parts(x)[1])\n",
    "\n",
    "parliament['sentiment_bert_label'] = parliament['sentiment_bert'].apply(lambda x: extract_sentiment_parts(x)[0])\n",
    "parliament['sentiment_bert_score'] = parliament['sentiment_bert'].apply(lambda x: extract_sentiment_parts(x)[1])\n",
    "\n",
    "# Display a sample of the results\n",
    "print(\"Generated data sample:\")\n",
    "print(generated[['sentiment_bert_label', 'sentiment_bert_score']].head())\n",
    "\n",
    "print(\"\\nOriginal data sample:\")\n",
    "print(parliament[['sentiment_bert_label', 'sentiment_bert_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d3236",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b6fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_descriptives' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# funciton defined above\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sentiment_vader_parliament \u001b[38;5;241m=\u001b[39m get_descriptives(parliament[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# boolean mask defined above\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sentiment_roberta_deepseek \u001b[38;5;241m=\u001b[39m get_descriptives(df_generated[is_deepseek][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_descriptives' is not defined"
     ]
    }
   ],
   "source": [
    "# funciton defined above\n",
    "sentiment_roberta_parliament = get_descriptives(parliament['sentiment_score'])\n",
    "# boolean mask defined above\n",
    "sentiment_roberta_deepseek = get_descriptives(generated[is_deepseek]['sentiment_score'])\n",
    "sentiment_roberta_chatgpt = get_descriptives(generated[is_openai]['sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d13829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and format table\n",
    "sentiment_table = PrettyTable()\n",
    "sentiment_table.field_names = ['Source', 'Mean', 'Std Dev', 'Min', 'Max', 'Count']\n",
    "\n",
    "sentiment_table.add_row(['Original', *sentiment_roberta_parliament])\n",
    "sentiment_table.add_row(['ChatGPT', *sentiment_roberta_chatgpt])\n",
    "sentiment_table.add_row(['DeepSeek', *sentiment_roberta_deepseek])\n",
    "\n",
    "# Print table\n",
    "print('Sentiment Analysis Results:')\n",
    "print(sentiment_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8458be56eaaaf",
   "metadata": {},
   "source": [
    "### Extremity Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7756928fcb574",
   "metadata": {},
   "source": [
    "##### Ridge Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485f127e80590c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_preprocess_text(dataset):\n",
    "    return dataset.map(\n",
    "        lambda x: ' '.join(\n",
    "            preprocessor.lemmatize_doc(\n",
    "                preprocessor.tokenize_doc(\n",
    "                    preprocessor.rm_stopwords(\n",
    "                        preprocessor.basic_clean(x)  # Remove ['translated_text'] access\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "X = parliament['translated_text']\n",
    "X = reg_preprocess_text(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749857d92889254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = parliament['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a469ad71d418e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b27c6cb72a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3222a9810b3f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge regression model\n",
    "model = Ridge()\n",
    "model.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0992a01f80f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb623a485ff305c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a90ce04603345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.plot([-1, 1], [-1, 1], '--', color='gray')\n",
    "plt.title(\"Predicted vs. Actual Extremity\")\n",
    "plt.xlabel(\"Actual Extremity\")\n",
    "plt.ylabel(\"Predicted Extremity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150e0336eaed957",
   "metadata": {},
   "source": [
    "##### RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5b9a881748e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a47cf77564475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = parliament[parliament[\"translated_text\"].notna() & parliament[\"label\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e203fe2e6de8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a145f9ecc37c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_df = train_df.rename(columns={\"label\": \"labels\"})\n",
    "test_df = test_df.rename(columns={\"label\": \"labels\"})\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"translated_text\", \"labels\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"translated_text\", \"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ea6a7068d185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RoBERTa for regression\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139fb00a1b0e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"translated_text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0a6507abd7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821725637da752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for regression\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\"\n",
    ")\n",
    "model.config.hidden_dropout_prob = 0.3  # reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388eee1a053c5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.squeeze()\n",
    "    return {\n",
    "        \"mse\": mean_squared_error(labels, preds),\n",
    "        \"r2\": r2_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66366f26362583ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./spectrum_bert_results\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./spectrum_logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7867f99bb46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c166a9b532ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9094807a5d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73412eb9c2bba82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"roberta_best\")\n",
    "tokenizer.save_pretrained(\"roberta_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496fdf3d9dfc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "statements = generated[\"statement\"].tolist()\n",
    "\n",
    "model_path = \"roberta_best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "extremity_scores = []\n",
    "for text in statements:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        score = outputs.logits.item()\n",
    "        extremity_scores.append(score)\n",
    "\n",
    "generated[\"extremity_score\"] = extremity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a239791a0cab825",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(data=generated, x=\"extremity_score\", hue=\"provider\", fill=True, common_norm=False, alpha=0.5)\n",
    "plt.axvline(0, linestyle=\"--\", color=\"gray\")\n",
    "plt.title(\"Distribution of Predicted Extremity Scores by Provider\")\n",
    "plt.xlabel(\"Extremity Score (-1 = Left, +1 = Right)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b9b26c27d1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=generated, x=\"provider\", y=\"extremity_score\")\n",
    "plt.title(\"Extremity Score Distribution per LLM\")\n",
    "plt.axhline(0, linestyle=\"--\", color=\"gray\")\n",
    "plt.ylabel(\"Predicted Extremity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be249b3713e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated.groupby(\"provider\")[\"extremity_score\"].agg([\"mean\", \"std\", \"min\", \"max\", \"median\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f67b0ebf135e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_df = generated[generated[\"provider\"].str.lower() == \"chatgpt\"]\n",
    "num_left_leaning = (chatgpt_df[\"extremity_score\"] < 0).sum()\n",
    "total = len(chatgpt_df)\n",
    "print(f\"ChatGPT statements leaning left (< 0): {num_left_leaning} out of {total} ({(num_left_leaning/total)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23febb39a64698e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
